# nequi-ds-compliance-risk

# üèóÔ∏è 1. Arquitectura Anal√≠tica en AWS para Gesti√≥n Integral de Riesgos
## An√°lisis Din√°mico de Riesgos Corporativos

Este repositorio documenta el **dise√±o de una arquitectura anal√≠tica conceptual en AWS** orientada a la **gesti√≥n integral de riesgos**, soportando an√°lisis din√°mico, modelos de Machine Learning, c√°lculos actuariales b√°sicos y un fuerte enfoque en **trazabilidad, explicabilidad y gobierno del dato**.

---

## üéØ Objetivos del Sistema

- Centralizar y gobernar informaci√≥n de riesgo proveniente de m√∫ltiples fuentes.
- Dise√±ar y mantener una **Matriz de Riesgo Corporativa** actualizable din√°micamente.
- Desarrollar **modelos de Machine Learning** para estimar la **probabilidad de eventos de riesgo**.
- Ejecutar **c√°lculos actuariales simples** (frecuencia, severidad, p√©rdida esperada, reservas y escenarios).
- Garantizar **trazabilidad end-to-end**, **explicabilidad de modelos** y **cumplimiento regulatorio**.


---

## üß© Diagrama de Arquitectura

![alt text](image.png)

![p1](https://github.com/user-attachments/assets/1879f646-76a3-488e-a644-a75184a1a690)

---

## üó∫Ô∏è Descripci√≥n de la Arquitectura por Fases

### 1Ô∏è‚É£ Fuentes de Datos
- Incidentes operativos  
- Sanciones regulatorias (PDF / XML)  
- PQRs (texto y audio WAV)  
- Exposici√≥n por producto  
- Eventos cr√≠ticos (streaming)

Estas fuentes constituyen el origen de los eventos de riesgo y la exposici√≥n utilizada en la matriz de riesgo, los modelos predictivos y los c√°lculos actuariales.

---

### 2Ô∏è‚É£ Ingesta (Batch y Streaming)

**Batch**
- AWS DMS  
- AWS Transfer Family (SFTP)  
- AWS DataSync  
- API Gateway + WAF  

**Streaming**
- Kinesis Data Streams  
- Kinesis Firehose  
- EventBridge  

**Orquestaci√≥n**
- AWS Step Functions

Esta capa garantiza una ingesta segura, desacoplada y totalmente trazable.

---

### 3Ô∏è‚É£ Data Lake (Bronze / Silver / Gold)

- **Bronze**: datos crudos e inmutables  
- **Silver**: datos limpios y estandarizados  
- **Gold**: datasets anal√≠ticos y de consumo  

Tecnolog√≠as:
- Amazon S3  
- Apache Iceberg (ACID, time-travel)  
- AWS Glue Data Catalog  

Aqu√≠ se almacenan la **matriz de riesgo corporativa**, resultados actuariales y features de ML.

---

### 4Ô∏è‚É£ Parsing y Enriquecimiento

- Textract (PDFs)  
- Transcribe (audio)  
- Comprehend (NLP)  
- AWS Lambda  

Convierte datos no estructurados en informaci√≥n anal√≠tica.

---

### 5Ô∏è‚É£ Calidad y Observabilidad

- Glue Data Quality / Deequ  
- Alertas SNS / Slack / Webhook  

Las m√©tricas de calidad se almacenan en un **DQ Mart** consultable v√≠a Athena o Redshift.

---

### 6Ô∏è‚É£ C√≥mputo y Anal√≠tica

- AWS Glue  
- Amazon EMR  
- Amazon Athena  
- Amazon Redshift  

Aqu√≠ se calculan:
- Matriz de riesgo corporativa  
- C√°lculos actuariales  
- Data marts anal√≠ticos  

---

### 7Ô∏è‚É£ ML / MLOps

- Feature Store  
- SageMaker Pipelines  
- SageMaker Training  
- Model Registry  
- Batch Transform / Endpoints  
- Model Monitor  
- SageMaker Clarify  

Permite estimar probabilidades de eventos con explicabilidad y control de sesgo.

---

### 8Ô∏è‚É£ Consumo y Reporting

- Amazon QuickSight  
- Reportes regulados (PDF / CSV)  

Entrega informaci√≥n a comit√© de riesgos y auditor√≠a.

---

### 9Ô∏è‚É£ Gobierno y Seguridad

- Lake Formation  
- KMS  
- Secrets Manager  
- CloudTrail  
- AWS Config  
- Security Hub  
- GuardDuty  
- Macie  
- CloudWatch  
- VPC / Endpoints  

Garantiza cumplimiento, seguridad y trazabilidad completa.

---

## üîÑ Flujo de Datos (Resumen)

1. Fuentes ‚Üí Ingesta ‚Üí S3 Bronze  
2. Parsing / Calidad ‚Üí S3 Silver  
3. Anal√≠tica / ML ‚Üí S3 Gold / Redshift  
4. Consumo ‚Üí Dashboards y reportes  

---

## ‚úÖ Consideraciones de Dise√±o

- Arquitectura event-driven
- Separaci√≥n Bronze / Silver / Gold
- Time-travel y reproducibilidad con Iceberg
- Seguridad y gobierno alineados a entornos regulados
 
# 2. Matriz de Riesgo + ML Supervisado

Dataset esperado:

- `risk_id`, `process`, `risk_type`, `events_last_3y`, `total_loss_last_3y`, `controls_level`, `critical_flag`

Parte A ‚Äì Matriz de Riesgo:

- Calcular frecuencia anual (`events_last_3y / years_window`)
- Calcular severidad promedio (`total_loss_last_3y / events_last_3y`, safe divide)
- Calcular p√©rdida esperada anual (freq * severity)
- Asignar probabilidad 1‚Äì5 (m√©todo `quantile` o `fixed` seg√∫n `RiskMatrixConfig`)
- Asignar impacto 1‚Äì5 (m√©todo `quantile` o `fixed`)
- Construir matriz 5x5 (conteos y expected loss)
- Agregar por `process` y `risk_type` (agregaciones clave)
- Incluir tests unitarios para cada componente (m√©tricas, scoring, matriz, agregaciones)

Parte B ‚Äì Modelo Supervisado:

- Entrenar un modelo simple para predecir `critical_flag` (baseline: regresi√≥n log√≠stica, alternativa: RandomForest o XGBoost)
- Explicar features y proceso: uso `FeatureSpec`, preprocesador (imputaci√≥n, OHE), y t√©cnicas de explicaci√≥n ligera (coeficientes, permutation importance; opcional: SHAP)
- M√©tricas: accuracy, precision, recall, f1, ROC-AUC y PR-AUC; priorizar PR-AUC en datasets desbalanceados
- Manejo de desbalanceo: `class_weight='balanced'`, opci√≥n `SMOTE` dentro de pipeline, o `scale_pos_weight` para XGBoost

Estructura del proyecto:

- `risk_matrix/` : implementaci√≥n de la Matriz de Riesgo
	- `src/` : c√≥digo (pipeline, metrics, scoring, matrix, aggregation, config)
	- `tests/` : tests de unidad para la matriz
- `supervised_model/` : pipeline de ML supervisado
	- `src/ml/` : `features.py`, `train.py`, `model.py` (ModelConfig)
	- `src/cli.py` : CLI para entrenar y guardar artefactos
	- `tests/` : tests de unidad para el entrenamiento
- `data/raw/` : datasets de ejemplo (`dataset_dummy_compliance.csv`)
- `files/` : outputs generados por los CLIs (metrics, modelos, PNGs)

C√≥mo ejecutar (PowerShell):

- Instalar dependencias y activar el virtualenv (si no est√° creado):
```powershell
python -m venv .venv
.\\.venv\\Scripts\\Activate.ps1
pip install -r requirements.txt
```  

```  

- Ejecutar tests (desde la ra√≠z del repo):
```powershell
# Ejecuta todos los tests
python -m pytest -q

# Ejecuta solo tests de risk_matrix
python -m pytest risk_matrix/tests/test_matrix.py -q

# Ejecuta solo tests de supervised_model
$env:PYTHONPATH="supervised_model"
python -m pytest supervised_model/tests/test_ml_train.py -q
```

- Pipeline Matriz de Riesgo (genera CSVs en `files/output`):
```powershell
# desde la ra√≠z del repo
.\\.venv\\Scripts\\python.exe -m risk_matrix.src.cli --input data/raw/dataset_dummy_compliance.csv --out files/output --prob-method quantile --impact-method quantile
```

- Entrenar modelo supervisado (usa el CSV generado o el raw si ya contiene las columnas opcionales):
```powershell
# entrenar con CSV enriquecido
$env:PYTHONPATH="supervised_model"
.\\.venv\\Scripts\\python.exe -m src.cli --input files/output/risk_metrics.csv --out files/ml_output --model-type logreg

# usar XGBoost
.\\.venv\\Scripts\\python.exe -m src.cli --input files/output/risk_metrics.csv --out files/ml_output_xgb --model-type xgb
```

- Opci√≥n para ejecutar internamente la generaci√≥n de metrics antes del entrenamiento:
```powershell
# Ejecuta risk_matrix primero y luego entrena
.\\.venv\\Scripts\\python.exe -m risk_matrix.src.cli --input data/raw/dataset_dummy_compliance.csv --out files/output
$env:PYTHONPATH="supervised_model"
.\\.venv\\Scripts\\python.exe -m src.cli --input files/output/risk_metrics.csv --out files/ml_output --model-type rf

``` 


# 3. Optimizaci√≥n Bajo Restricciones

**Contexto**
- Score $s\in[0,1]$ por registro. Clasificaci√≥n objetivo: `bajo` (70%), `medio` (20%), `alto` (10%).
- Restricciones operacionales: maximizar detecci√≥n de `cr√≠ticos` dentro de `alto`, mantener falsos positivos (FP) en `alto` < 15%, no se puede reentrenar el modelo (solo ajustar cortes).

**Notaci√≥n y funci√≥n objetivo**
- Sea $s_i$ el score y $y_i\in\{0,1\}$ la etiqueta (1=cr√≠tico). Definimos dos umbrales $t_1,t_2$ con $0\le t_1<t_2\le1$:
	- `bajo`: $s<t_1$ (‚âà70%)
	- `medio`: $t_1\le s<t_2$ (‚âà20%)
	- `alto`: $s\ge t_2$ (‚âà10%)
- Objetivo: maximizar el recall de cr√≠ticos ubicados en `alto`:
$$\max_{t_1,t_2} R_{alto}(t_2)=\frac{\sum_i \mathbb{1}(y_i=1)\,\mathbb{1}(s_i\ge t_2)}{\sum_i \mathbb{1}(y_i=1)}$$

**Restricciones**
- Proporciones objetivo (con tolerancia $\epsilon$, p.ej. $\epsilon=0.02$):
$$\frac{1}{N}\sum_i\mathbb{1}(s_i<t_1)\approx0.70,\quad\frac{1}{N}\sum_i\mathbb{1}(t_1\le s_i<t_2)\approx0.20,\quad\frac{1}{N}\sum_i\mathbb{1}(s_i\ge t_2)\approx0.10$$
- Falsos positivos en `alto`:
$$\frac{FP_{alto}}{N_{non}}\le 0.15$$
- Dominio: $0\le t_1<t_2\le1$. Solo se permiten cambios en los cortes (no reentrenamiento).

**M√©todo de optimizaci√≥n (pr√°ctico y explicable)**
- Recomendado: b√∫squeda de cuadr√≠cula (grid-search) sobre percentiles de score + reglas de selecci√≥n.
	1. Punto de partida: $t_1^{(0)}=Q_{0.70}$, $t_2^{(0)}=Q_{0.90}$.
	2. Definir grid en percentiles (p.ej. p1=50..80, p2=85..99). Para cada par (p1,p2) calcular $t_1,t_2$ y m√©tricas: `recall_high`, `fp_rate_high`, proporciones.
	3. Filtrar soluciones que cumplan restricciones (proporciones dentro de tolerancia y `fp_rate_high` ‚â§ 0.15).
	4. Seleccionar la soluci√≥n factible con mayor `recall_high`. Si no existe, seleccionar la que cumpla `fp_rate_high` y maximice `recall_high`, o presentar trade-off.
	5. Alternativa: formulaci√≥n con penalizaci√≥n:
	$$\max_{t_1,t_2}\;R_{alto}(t_2)-\lambda\max(0,FP_{alto}/N_{non}-0.15)-\mu\,penalty\,,$$
	optimizado por m√©todos de caja negra (COBYLA) o grid-search; preferir grid-search por auditabilidad.

**Validaci√≥n y control**
- Hold-out temporal o estratificado distinto al que se us√≥ para selecci√≥n. Nunca validar sobre el mismo conjunto de ajuste.
- Backtesting: aplicar umbrales en ventanas hist√≥ricas para medir estabilidad y deriva.
- Bootstrap (p.ej. 500‚Äì1000 resamples) para estimar intervalos de confianza de `recall_high` y `fp_rate_high`.
- Curvas de sensibilidad: variar $t_2$ y $t_1$ alrededor de la soluci√≥n y mostrar efecto en `recall_high` vs `fp_rate_high`.
- M√©tricas a reportar: `recall_high`, `precision_high`, `fp_rate_high`, proporciones reales (bajo/medio/alto), PR-AUC y ROC-AUC globales.
- Controles de fairness: checks por subgrupo y revisi√≥n manual de muestras en `alto` y falsos positivos.

**Implementaci√≥n operativa**
- Ejecutar script de optimizaci√≥n sobre dataset de validaci√≥n (ejemplo: `tools/threshold_optimizer.py`).
- Guardar artefactos: `threshold_best.json`, `threshold_grid_results.csv`, `annotated_scores.csv`, im√°genes de trade-off y bootstrap.
- Versionar el paquete de configuraci√≥n (umbral, dataset usado, fecha, m√©tricas) y exigir aprobaci√≥n de Riesgos + Legal antes de producci√≥n.
- Monitor diario en producci√≥n: `FP_rate_alto`, `Recall_alto`, volumen de casos `alto`; rollback autom√°tico si `FP_rate_alto` excede 15%.

**Comunicaci√≥n a Riesgos / Legal / Regulador (texto breve listo para presentar)**
- Objetivo: Ajustar √∫nicamente los umbrales de clasificaci√≥n del score para priorizar la detecci√≥n de casos cr√≠ticos en la categor√≠a `alto`. No se reentrena ni modifica el modelo base.
- Beneficios: intervenci√≥n r√°pida, auditable y reversible que permite concentrar la revisi√≥n manual en el 10% m√°s alto del score.
- Garant√≠as: todos los umbrales se validan con hold-out y backtesting; se calculan intervalos por bootstrap; se registra versi√≥n y dataset; cualquier cambio requiere aprobaci√≥n formal.
- Reglas operativas:
	- Umbrales propuestos se evaluar√°n en entorno de pruebas durante 1 mes con monitoreo diario.
	- Si durante producci√≥n `FP_rate_alto` > 15%, se revierte el cambio e inicia investigaci√≥n.
	- Se mantendr√° evidencia para auditor√≠a (dataset, c√≥digo, versi√≥n de umbrales, registros de aprobaci√≥n).
- Riesgos y trade-offs: mejorar `recall_high` puede aumentar carga operativa por revisiones manuales; presentamos curvas de trade-off y estimaciones de impacto operativo.

---

# 4. Problema de Escenario ‚Äì Gobernanza y Conflictos

### Escenario
Las √°reas de **Riesgos**, **Legal** y **Actuar√≠a** identifican discrepancias entre los criterios tradicionales (reglas expertas, matrices est√°ticas, supuestos actuariales) y los resultados generados por un **modelo de Machine Learning** para evaluaci√≥n de riesgo. Estas diferencias generan fricci√≥n operativa, dudas sobre la validez del modelo y riesgos de cumplimiento regulatorio.  

---

### ¬øC√≥mo gestionar desacuerdos entre estos frentes?

- Establecer un **foro formal de decisi√≥n** (Risk & Model Committee) con representantes de Riesgos, Legal, Actuar√≠a, Data Science y Compliance.
- Separar claramente **roles y responsabilidades**:
  - Riesgos: apetito de riesgo y uso del output.
  - Actuar√≠a: coherencia estad√≠stica y supuestos.
  - Data Science: desempe√±o, estabilidad y sesgo del modelo.
  - Legal/Compliance: alineaci√≥n regulatoria y trazabilidad.
- Utilizar **evidencia cuantitativa** para la discusi√≥n:
  - Comparaci√≥n ML vs. criterios tradicionales (backtesting).
  - An√°lisis de impacto en m√©tricas clave (p√©rdida esperada, reservas, solvencia).
- Documentar expl√≠citamente **excepciones y overrides**, evitando decisiones ad-hoc.

---

### ¬øC√≥mo definir gobernanza?

- Implementar un **framework de gobernanza de modelos** que incluya:
  - Ciclo de vida del modelo (dise√±o, validaci√≥n, despliegue, monitoreo, retiro).
  - Validaci√≥n independiente (Model Validation / Second Line).
  - KPIs de seguimiento: performance, estabilidad, drift, explainability.
- Definir al ML como **modelo complementario**, no sustitutivo:
  - ML apoya la toma de decisiones.
  - La decisi√≥n final permanece bajo responsabilidad del negocio.
- Mantener **versionamiento y control de cambios**:
  - Cambios en features, bins, thresholds o datasets deben ser aprobados y auditables.

---

### ¬øC√≥mo explicar al regulador?

- Presentar el modelo como un **mecanismo de apoyo a la gesti√≥n del riesgo**.
- Enfatizar:
  - Explicabilidad del modelo (features, importancia, l√≥gica de scoring).
  - Consistencia con criterios tradicionales (alineaci√≥n conceptual).
  - Evidencia emp√≠rica de mejora (menor error, mejor detecci√≥n, reducci√≥n de p√©rdidas).
- Proveer documentaci√≥n clara:
  - Supuestos del modelo.
  - Limitaciones conocidas.
  - Controles implementados (monitoreo, overrides, revisi√≥n humana).
- Demostrar que existe **gobernanza activa**, trazabilidad y accountability, alineada con principios de gesti√≥n prudencial y regulatoria.

---

**Conclusi√≥n:** 
La decisi√≥n no es imponer el modelo ML, sino integrarlo dentro de una gobernanza s√≥lida, donde act√∫e como una herramienta cuantitativa adicional, validada, explicable y supervisada, capaz de convivir con los enfoques tradicionales y cumplir con las expectativas regulatorias.
